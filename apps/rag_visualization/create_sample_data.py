"""
Create sample data for RAG Memory Service visualization.
Populates the database with interesting interconnected memories.
"""
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from services.rag_memory import RagMemoryService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_programming_memories(service: RagMemoryService):
    """Create memories about programming languages and concepts."""
    logger.info("Creating programming memories...")
    
    memories = [
        "Python is a high-level, interpreted programming language known for its simplicity and readability. Created by Guido van Rossum and first released in 1991, Python emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a large and comprehensive standard library, often described as 'batteries included'. Python interpreters are available for many operating systems, making it highly portable. The language is dynamically typed and garbage-collected, which simplifies memory management. Python is widely used in web development, data science, artificial intelligence, scientific computing, and automation. Popular frameworks like Django and Flask make it a strong choice for backend development. The language's syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.",
        "JavaScript is the language of the web, running in browsers and servers via Node.js. Originally created by Brendan Eich in 1995, JavaScript has evolved from a simple scripting language for web pages to a powerful, versatile programming language. It is a high-level, just-in-time compiled language that conforms to the ECMAScript specification. JavaScript supports event-driven, functional, and imperative programming styles. It has application programming interfaces (APIs) for working with text, dates, regular expressions, standard data structures, and the Document Object Model (DOM). Modern JavaScript includes features like arrow functions, destructuring, async/await, classes, modules, and template literals. Node.js enables JavaScript to run on the server side, creating full-stack applications with a single language. The npm ecosystem is the largest software registry in the world, providing millions of packages. JavaScript frameworks like React, Vue, and Angular have revolutionized frontend development. The language's asynchronous nature makes it ideal for handling I/O operations and building responsive user interfaces.",
        "TypeScript adds static typing to JavaScript, making code more maintainable and scalable. Developed by Microsoft and released in 2012, TypeScript is a superset of JavaScript that compiles to plain JavaScript. It provides optional static type checking, which helps catch errors during development rather than at runtime. TypeScript supports interfaces, classes, generics, and advanced type features like union types, intersection types, and conditional types. The language offers excellent tooling support with IntelliSense, refactoring capabilities, and better IDE integration. TypeScript's type system is structural rather than nominal, meaning types are compatible if they have the same structure. It supports modern JavaScript features and can target different ECMAScript versions. TypeScript is widely adopted in large-scale applications where type safety reduces bugs and improves code quality. Popular frameworks like Angular are built with TypeScript, and React and Vue have excellent TypeScript support. The language's gradual typing allows developers to adopt it incrementally in existing JavaScript projects. TypeScript's compiler provides detailed error messages that help developers understand and fix issues quickly.",
        "Rust provides memory safety without garbage collection through ownership. Created by Graydon Hoare at Mozilla Research, Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety. Rust achieves memory safety through its ownership system, which tracks variable lifetimes and prevents data races at compile time. The language has three core concepts: ownership, borrowing, and lifetimes. Ownership rules ensure that each value has exactly one owner, and when the owner goes out of scope, the value is dropped. Borrowing allows references to data without taking ownership, with the compiler enforcing that references cannot outlive the data they reference. Rust's zero-cost abstractions mean that high-level code compiles to machine code as efficient as hand-written C. The language includes pattern matching, algebraic data types, and powerful error handling with Result and Option types. Rust's package manager, Cargo, handles building, testing, and dependency management. The language is increasingly used in systems programming, web assembly, embedded systems, and performance-critical applications. Rust's strong type system and compiler checks eliminate entire classes of bugs common in other systems languages.",
        "Go is designed for concurrent programming and scalability at Google. Created by Robert Griesemer, Rob Pike, and Ken Thompson at Google in 2009, Go is a statically typed, compiled programming language. The language was designed to address common criticisms of other languages while maintaining their positive characteristics. Go has a simple, clean syntax that is easy to learn and read. It includes built-in support for concurrent programming through goroutines and channels, making it easy to write programs that get the most out of multicore and networked machines. Goroutines are lightweight threads managed by the Go runtime, and channels provide a way for goroutines to communicate and synchronize. Go's garbage collector is efficient and designed for low latency. The language compiles quickly to machine code, making it suitable for large codebases. Go has a strong standard library that includes packages for networking, cryptography, and web development. The language's tooling is excellent, with built-in formatting, testing, and documentation tools. Go is widely used in cloud computing, microservices, and distributed systems. Companies like Docker, Kubernetes, and many others have built critical infrastructure using Go.",
        "Java is a class-based, object-oriented language that runs on the JVM. Developed by James Gosling at Sun Microsystems in 1995, Java was designed to have as few implementation dependencies as possible. The language follows the 'write once, run anywhere' principle, meaning compiled Java code can run on all platforms that support Java without recompilation. Java applications are typically compiled to bytecode that can run on any Java Virtual Machine (JVM) regardless of the underlying computer architecture. The language is strongly typed, meaning variables must be declared with a specific type. Java's object-oriented programming model includes concepts like classes, objects, inheritance, polymorphism, and encapsulation. The language has automatic memory management through garbage collection. Java includes a comprehensive standard library and a vast ecosystem of frameworks and tools. Popular frameworks like Spring, Hibernate, and Apache Struts are widely used in enterprise development. Java is commonly used for building large-scale enterprise applications, Android mobile apps, and web applications. The language's robustness, security, and portability make it a popular choice for mission-critical systems.",
        "C++ is a powerful systems programming language with manual memory management. Created by Bjarne Stroustrup at Bell Labs in 1979, C++ is an extension of the C programming language with object-oriented, generic, and functional features. The language gives programmers low-level control over system resources and memory, making it suitable for performance-critical applications. C++ supports multiple programming paradigms, including procedural, object-oriented, generic, and functional programming. The language includes features like classes, templates, operator overloading, and exception handling. C++'s Standard Template Library (STL) provides containers, algorithms, and iterators that simplify common programming tasks. The language is widely used in game development, operating systems, embedded systems, and high-performance computing. Modern C++ (C++11 and later) includes features like auto type deduction, lambda expressions, smart pointers, and range-based for loops. C++ compiles to native machine code, providing excellent performance. The language's complexity allows for highly optimized code but requires careful memory management to avoid leaks and undefined behavior. C++ is the foundation for many other languages and remains one of the most widely used programming languages in the world.",
        "Swift is Apple's modern language for iOS and macOS development. Introduced by Apple in 2014, Swift was designed to replace Objective-C while maintaining interoperability with existing Cocoa and Cocoa Touch frameworks. Swift is a general-purpose, multi-paradigm, compiled programming language that combines the best ideas from various programming languages. The language emphasizes safety, performance, and expressiveness. Swift's type system helps prevent common programming errors, and its syntax is clean and expressive. The language includes modern features like optionals, closures, generics, and protocol-oriented programming. Swift's memory management uses Automatic Reference Counting (ARC), which automatically manages memory without the overhead of garbage collection. The language supports functional programming patterns and has powerful pattern matching capabilities. Swift Playgrounds provide an interactive environment for learning and prototyping. The language is open-source and has been ported to Linux and other platforms. Swift's performance is comparable to C++ for many tasks, making it suitable for system programming as well as application development. The language's safety features, combined with its modern syntax, make it an excellent choice for developing robust applications.",
        "Kotlin is a modern language for Android development, interoperable with Java. Developed by JetBrains and first released in 2011, Kotlin is a statically typed programming language that runs on the Java Virtual Machine. Google announced official support for Kotlin on Android in 2017, making it a first-class language for Android development. Kotlin is designed to be fully interoperable with Java, meaning existing Java code can be called from Kotlin and vice versa. The language addresses many of the verbosity and safety issues found in Java. Kotlin includes null safety features that help prevent NullPointerExceptions, a common source of bugs in Java applications. The language has concise syntax, reducing boilerplate code significantly compared to Java. Kotlin supports functional programming with features like lambda expressions, higher-order functions, and extension functions. The language includes coroutines for asynchronous programming, making it easier to write concurrent code. Kotlin can also compile to JavaScript and native code, expanding its use cases beyond Android development. The language's data classes, sealed classes, and when expressions provide powerful pattern matching capabilities. Kotlin's modern features and Java interoperability make it an excellent choice for both new projects and migrating existing Java codebases.",
        "Ruby is known for its elegant syntax and the Rails web framework. Created by Yukihiro Matsumoto in 1995, Ruby is a dynamic, reflective, object-oriented programming language. The language emphasizes developer happiness and productivity, with a philosophy that values human needs over machine needs. Ruby's syntax is designed to be natural and intuitive, often reading like English. The language follows the principle of least surprise, meaning it behaves in ways that minimize confusion for experienced programmers. Ruby on Rails, commonly known as Rails, is a server-side web application framework written in Ruby. Rails follows the convention over configuration philosophy, reducing the number of decisions developers need to make. The framework includes features like ActiveRecord for database interactions, ActionController for handling requests, and ActionView for templating. Ruby's metaprogramming capabilities allow developers to write code that writes code, enabling powerful DSLs (Domain-Specific Languages). The language has a vibrant ecosystem with the RubyGems package manager providing thousands of libraries. Ruby's dynamic nature and duck typing make it flexible but require careful testing. The language is widely used in web development, automation, and prototyping. Ruby's focus on developer experience has influenced many other programming languages and frameworks.",
        "C# is a modern, object-oriented programming language developed by Microsoft. Created by Anders Hejlsberg and his team, C# was first released in 2000 as part of Microsoft's .NET initiative. The language combines the productivity of high-level languages with the power and control of C++. C# is strongly typed and supports multiple programming paradigms, including object-oriented, functional, and component-oriented programming. The language includes features like properties, events, delegates, and attributes that make it well-suited for component-based development. C# has automatic garbage collection, exception handling, and type safety features. The language's LINQ (Language Integrated Query) provides a powerful way to query data from various sources. C# is widely used for developing Windows applications, web applications using ASP.NET, and games using Unity. The language has evolved significantly, with modern versions including features like async/await, pattern matching, and nullable reference types. C#'s integration with the .NET ecosystem provides access to a vast library of frameworks and tools. The language's syntax is similar to C, C++, and Java, making it easy for developers familiar with those languages to learn. C# continues to evolve with regular updates that add new features and improve performance.",
        "PHP is a popular server-side scripting language designed for web development. Created by Rasmus Lerdorf in 1994, PHP originally stood for Personal Home Page but now stands for PHP: Hypertext Preprocessor. PHP is embedded in HTML and executed on the server, generating dynamic web page content. The language is particularly well-suited for web development and can be embedded into HTML. PHP has a large ecosystem with frameworks like Laravel, Symfony, and CodeIgniter that provide structure and tools for building web applications. The language supports object-oriented programming, functional programming, and procedural programming styles. PHP has built-in support for many databases, including MySQL, PostgreSQL, and SQLite. The language's ease of use and extensive documentation make it accessible to beginners. PHP powers a significant portion of the web, including major platforms like WordPress, Drupal, and Facebook. Modern PHP (version 7 and later) has significantly improved performance and includes features like type declarations, return type declarations, and anonymous classes. PHP's extensive standard library provides functions for common tasks like string manipulation, file handling, and database operations. The language's flexibility and wide hosting support make it a popular choice for web development.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["programming"] * len(memories),
        sources=["documentation"] * len(memories)
    )


def create_ai_ml_memories(service: RagMemoryService):
    """Create memories about AI and machine learning."""
    logger.info("Creating AI/ML memories...")
    
    memories = [
        "Machine learning is a subset of AI that learns patterns from data without being explicitly programmed. Unlike traditional programming where rules are hardcoded, machine learning algorithms build mathematical models based on training data to make predictions or decisions. The field encompasses supervised learning, where models learn from labeled examples, unsupervised learning, which finds patterns in unlabeled data, and reinforcement learning, where agents learn through interaction with an environment. Machine learning has applications in image recognition, natural language processing, recommendation systems, fraud detection, and autonomous vehicles. The process typically involves data collection, feature engineering, model selection, training, validation, and deployment. Popular algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks. The success of machine learning depends heavily on the quality and quantity of training data. Overfitting, where models memorize training data but fail to generalize, is a common challenge. Regularization techniques and cross-validation help prevent overfitting. Machine learning has revolutionized many industries and continues to advance rapidly with new architectures and techniques.",
        "Deep learning uses neural networks with multiple layers for complex tasks. These deep neural networks can learn hierarchical representations of data, with lower layers detecting simple features and higher layers combining them into more complex patterns. Deep learning has achieved remarkable success in computer vision, natural language processing, speech recognition, and game playing. The field gained momentum with the availability of large datasets, powerful GPUs, and improved algorithms. Convolutional Neural Networks (CNNs) excel at processing grid-like data such as images, while Recurrent Neural Networks (RNNs) are designed for sequential data. Deep learning models can have hundreds or thousands of layers, enabling them to learn very complex patterns. Training deep networks requires careful initialization, appropriate activation functions, and optimization techniques like gradient descent variants. Techniques like dropout and batch normalization help stabilize training and improve generalization. Transfer learning allows practitioners to leverage pre-trained models, significantly reducing training time and data requirements. Deep learning has enabled breakthroughs in areas like medical diagnosis, autonomous driving, and language translation. The field continues to evolve with new architectures like transformers and attention mechanisms pushing the boundaries of what's possible.",
        "Neural networks are inspired by biological neurons in the human brain. Just as biological neurons receive inputs through dendrites, process them in the cell body, and send outputs through axons, artificial neurons receive inputs, apply weights and a bias, pass the result through an activation function, and produce an output. A single neuron is a simple computational unit, but when connected in layers, neural networks can approximate complex functions. The connections between neurons have weights that are adjusted during training to learn patterns in data. The activation function introduces non-linearity, allowing neural networks to model complex relationships. Feedforward neural networks pass information in one direction from input to output layers. Backpropagation is the algorithm used to train neural networks by calculating gradients and updating weights to minimize error. The universal approximation theorem states that a neural network with a single hidden layer can approximate any continuous function, though deeper networks are often more efficient. Neural networks can learn from examples without explicit programming of rules. They are particularly powerful when dealing with high-dimensional data like images, audio, and text. Modern neural networks have millions or billions of parameters, requiring significant computational resources for training.",
        "Transformers revolutionized NLP with attention mechanisms. Introduced in the paper 'Attention Is All You Need' in 2017, transformers replaced recurrent and convolutional layers with self-attention mechanisms. The key innovation is the attention mechanism, which allows the model to focus on different parts of the input sequence when processing each element. Unlike RNNs that process sequences sequentially, transformers process all positions in parallel, making training much faster. The transformer architecture consists of an encoder and decoder, each containing multiple layers of self-attention and feedforward networks. Positional encoding is added to input embeddings to provide information about token positions. Multi-head attention allows the model to attend to information from different representation subspaces simultaneously. Transformers have become the foundation for most state-of-the-art NLP models, including BERT, GPT, and T5. The architecture's parallelization capabilities and ability to capture long-range dependencies have made it the dominant approach in NLP. Transformers have also been successfully applied to computer vision tasks with Vision Transformers (ViTs). The attention mechanism provides interpretability, as attention weights can show which parts of the input the model focuses on. Pre-trained transformer models can be fine-tuned for specific tasks with relatively little data, making them highly practical for real-world applications.",
        "GPT models use transformer architecture for text generation. GPT stands for Generative Pre-trained Transformer, and these models are trained on vast amounts of text data to predict the next word in a sequence. The GPT architecture uses only the decoder part of the transformer, making it autoregressive - it generates text one token at a time based on previous tokens. GPT models are pre-trained on diverse internet text, learning grammar, facts, reasoning abilities, and even some degree of common sense. The models can then be fine-tuned for specific tasks or used in few-shot learning scenarios where examples are provided in the prompt. GPT-3, with 175 billion parameters, demonstrated remarkable few-shot learning capabilities, performing tasks it wasn't explicitly trained on. GPT-4 and later versions show improved reasoning, reduced hallucinations, and better instruction following. These models have applications in text completion, question answering, translation, summarization, and creative writing. The training process involves predicting the next token in sequences, which teaches the model to understand language structure and context. GPT models use techniques like temperature sampling and top-k sampling to control the randomness of generated text. The models have limitations including potential for generating false information, bias, and lack of real-time knowledge updates. Despite limitations, GPT models represent a significant advancement in natural language understanding and generation.",
        "BERT is a bidirectional transformer model for understanding context. Introduced by Google in 2018, BERT stands for Bidirectional Encoder Representations from Transformers. Unlike GPT models that process text left-to-right, BERT uses bidirectional context, meaning it reads text in both directions simultaneously. This bidirectional approach allows BERT to better understand the context of words by considering both preceding and following text. BERT is pre-trained using two tasks: Masked Language Modeling (MLM), where random words are masked and the model predicts them, and Next Sentence Prediction (NSP), where the model learns relationships between sentences. The model uses only the encoder part of the transformer architecture. BERT can be fine-tuned for various downstream tasks like sentiment analysis, named entity recognition, question answering, and text classification with minimal task-specific modifications. The model's contextualized word embeddings capture meaning based on surrounding context, solving the problem of polysemy where words have multiple meanings. BERT-base has 110 million parameters, while BERT-large has 340 million parameters. The model has been extended with variants like RoBERTa, ALBERT, and DistilBERT, each optimizing different aspects. BERT's bidirectional nature makes it particularly effective for understanding tasks but less suitable for generation tasks. The model has become a standard baseline for many NLP tasks and has influenced the development of subsequent models.",
        "Convolutional Neural Networks (CNNs) excel at image recognition. CNNs use convolutional layers that apply filters to input data, detecting features like edges, textures, and patterns. The convolution operation involves sliding a small filter over the input and computing dot products, which allows the network to detect features regardless of their position. Pooling layers reduce spatial dimensions, making the network more efficient and providing translation invariance. CNNs typically follow a pattern of convolutional layers, activation functions, pooling layers, and fully connected layers. The early layers detect low-level features like edges and corners, while deeper layers combine these into more complex patterns like shapes and objects. CNNs have achieved remarkable success in image classification, object detection, and image segmentation tasks. Architectures like LeNet, AlexNet, VGG, ResNet, and EfficientNet have pushed the boundaries of what's possible. Transfer learning with pre-trained CNNs allows practitioners to achieve good results with limited data by fine-tuning models trained on large datasets like ImageNet. CNNs are also applied to other domains like natural language processing for text classification and time series analysis. The architecture's parameter sharing through convolution makes it efficient and reduces overfitting. Data augmentation techniques like rotation, scaling, and cropping help improve generalization. CNNs have enabled applications in medical imaging, autonomous vehicles, and facial recognition systems.",
        "Recurrent Neural Networks (RNNs) process sequential data like text. Unlike feedforward networks, RNNs have connections that form cycles, allowing them to maintain hidden states that capture information about previous inputs. This makes RNNs naturally suited for tasks involving sequences, such as language modeling, machine translation, and speech recognition. The basic RNN processes sequences step by step, updating its hidden state at each time step based on the current input and previous hidden state. However, basic RNNs suffer from vanishing and exploding gradient problems when dealing with long sequences. Long Short-Term Memory (LSTM) networks address these issues with gating mechanisms that control information flow. LSTMs have forget gates, input gates, and output gates that allow them to selectively remember or forget information. Gated Recurrent Units (GRUs) are a simpler variant of LSTMs with fewer parameters. RNNs can be unidirectional, processing sequences left-to-right, or bidirectional, processing in both directions. Stacked RNNs with multiple layers can learn more complex patterns. RNNs have been largely superseded by transformers for many NLP tasks, but they remain useful for certain applications. The sequential nature of RNNs makes them slower to train than transformers, which can process sequences in parallel. RNNs are still commonly used in time series forecasting, where their ability to model temporal dependencies is valuable.",
        "Reinforcement learning trains agents through rewards and penalties. In reinforcement learning, an agent interacts with an environment, taking actions and receiving rewards or penalties based on those actions. The goal is to learn a policy that maximizes cumulative reward over time. The agent doesn't receive labeled examples like in supervised learning but must discover which actions lead to rewards through exploration. The environment provides feedback in the form of rewards, which can be sparse and delayed, making learning challenging. Key concepts include states (the current situation), actions (what the agent can do), rewards (feedback from the environment), and the policy (the strategy for choosing actions). Value functions estimate the expected future reward from a state or state-action pair. Q-learning is a popular algorithm that learns the value of state-action pairs. Deep Q-Networks (DQN) combine Q-learning with deep neural networks, enabling RL to work with high-dimensional state spaces. Policy gradient methods directly optimize the policy rather than learning value functions. Actor-critic methods combine value-based and policy-based approaches. Reinforcement learning has achieved remarkable success in game playing, with agents mastering games like Go, chess, and video games. Applications include robotics, autonomous driving, recommendation systems, and resource allocation. The exploration-exploitation trade-off is central to RL - agents must balance trying new actions with exploiting known good actions.",
        "Transfer learning reuses pre-trained models for new tasks. Instead of training models from scratch, transfer learning leverages knowledge learned from one task to improve performance on a related task. This is particularly valuable when you have limited data for your target task but access to large datasets for related tasks. The typical approach involves taking a model pre-trained on a large dataset, removing or modifying the final layers, and fine-tuning the remaining layers on your specific task. The early layers of neural networks often learn general features like edges and textures that are useful across many tasks, while later layers learn task-specific features. Transfer learning can dramatically reduce training time and data requirements. In computer vision, models pre-trained on ImageNet are commonly fine-tuned for specific image classification tasks. In NLP, models like BERT and GPT are pre-trained on large text corpora and then fine-tuned for tasks like sentiment analysis or question answering. Feature extraction is another approach where the pre-trained model is used as a fixed feature extractor, and only a new classifier is trained. Transfer learning has become standard practice in deep learning, enabling practitioners to achieve good results with limited resources. The technique works best when the source and target tasks are related, though some transfer can occur even between seemingly unrelated tasks. Transfer learning has democratized deep learning by making powerful models accessible to those without massive computational resources.",
        "Embeddings represent words or concepts as dense vectors. Unlike sparse one-hot encodings, embeddings map discrete objects like words to dense, continuous vector spaces where similar objects are close together. Word embeddings capture semantic and syntactic relationships - words with similar meanings have similar vectors, and relationships like 'king - man + woman â‰ˆ queen' can be represented as vector operations. Word2Vec, developed by Google, learns embeddings by predicting words from context or predicting context from words. GloVe (Global Vectors) combines global word co-occurrence statistics with local context window methods. FastText extends Word2Vec by representing words as bags of character n-grams, enabling it to handle out-of-vocabulary words. Contextualized embeddings like those from BERT and ELMo capture word meaning based on surrounding context, solving polysemy. Embeddings are typically learned during model training but can also be pre-trained and fine-tuned. The dimensionality of embeddings is a hyperparameter - higher dimensions can capture more information but require more data and computation. Embeddings enable mathematical operations on words, allowing models to reason about relationships. They're used in recommendation systems, search engines, and various NLP tasks. Sentence and document embeddings extend the concept to represent longer text. Embeddings have become fundamental to modern NLP, providing a way to represent text in a form that neural networks can process effectively.",
        "Vector databases enable fast similarity search for embeddings. Traditional databases are optimized for exact matches, but vector databases are designed for approximate nearest neighbor search, finding vectors similar to a query vector. This is essential for applications like semantic search, recommendation systems, and retrieval-augmented generation. Vector databases use specialized indexing structures like HNSW (Hierarchical Navigable Small World), IVF (Inverted File Index), or LSH (Locality-Sensitive Hashing) to enable fast similarity search even with millions or billions of vectors. Popular vector databases include Pinecone, Weaviate, Milvus, Qdrant, and pgvector (a PostgreSQL extension). These databases support operations like inserting vectors, querying for similar vectors, and filtering by metadata. The similarity metric, typically cosine similarity, dot product, or Euclidean distance, determines how vectors are compared. Vector databases are crucial for RAG systems, where relevant documents are retrieved based on semantic similarity to queries. They enable real-time search over large collections of embeddings, making them essential for production AI applications. Many vector databases support hybrid search, combining vector similarity with traditional keyword search. They often include features like automatic indexing, replication, and distributed architectures for scalability. Vector databases have become a critical infrastructure component for modern AI applications that need to search and retrieve relevant information quickly.",
        "RAG combines retrieval and generation for better AI responses. RAG, or Retrieval-Augmented Generation, addresses the limitations of language models by grounding their responses in retrieved documents. The process involves retrieving relevant documents from a knowledge base based on the user's query, then using those documents as context for the language model to generate a response. This approach provides several benefits: it reduces hallucinations by grounding responses in actual documents, allows the model to access up-to-date information without retraining, and enables the model to cite sources. RAG systems typically use vector databases to store document embeddings and retrieve semantically similar documents to queries. The retrieved documents are then passed to the language model along with the original query as context. RAG has become a standard approach for building AI assistants that need to answer questions about specific knowledge bases. The technique is particularly valuable for domain-specific applications where general language models lack sufficient knowledge. RAG systems can be enhanced with techniques like re-ranking retrieved documents, using multiple retrieval strategies, and iterative retrieval. The quality of RAG systems depends on the quality of the knowledge base, the retrieval mechanism, and the language model. RAG has enabled applications in customer support, legal research, medical information systems, and enterprise knowledge management. The approach represents a practical way to combine the reasoning capabilities of language models with external knowledge sources.",
        "Fine-tuning adapts pre-trained models to specific domains. While pre-trained models like GPT and BERT are powerful, they may not perform optimally on specific tasks or domains without adaptation. Fine-tuning involves continuing the training process on a smaller, task-specific dataset, allowing the model to adapt its parameters to the new task. This process is much more efficient than training from scratch, requiring less data, time, and computational resources. Fine-tuning can be done in several ways: full fine-tuning updates all model parameters, while parameter-efficient methods like LoRA (Low-Rank Adaptation) update only a small subset of parameters. The learning rate for fine-tuning is typically much smaller than for initial training to avoid catastrophic forgetting. Fine-tuning requires careful hyperparameter tuning, including learning rate, batch size, and number of epochs. Early stopping is often used to prevent overfitting on small datasets. Fine-tuning has enabled the application of large language models to specialized domains like legal, medical, and financial text. The technique is also used to adapt models to specific styles, tones, or formats. Fine-tuning can improve performance on downstream tasks while maintaining the general knowledge learned during pre-training. However, fine-tuning large models still requires significant computational resources. Recent advances in parameter-efficient fine-tuning have made it more accessible by reducing the computational requirements while maintaining performance.",
        "Prompt engineering optimizes inputs for language models. The way prompts are structured significantly affects the quality and relevance of model outputs. Prompt engineering involves designing and refining prompts to elicit desired behaviors from language models. Effective prompts provide clear instructions, relevant context, examples when helpful, and specify the desired output format. Techniques include few-shot learning, where examples are provided in the prompt to demonstrate the task, and chain-of-thought prompting, which encourages step-by-step reasoning. Prompt templates can be created for common tasks, ensuring consistency and quality. The order and phrasing of instructions can significantly impact results. Prompt engineering is particularly important for general-purpose models that need to be adapted to specific tasks without fine-tuning. Advanced techniques include role-playing prompts, where the model is asked to assume a specific role, and constraint-based prompts that limit the output space. Prompt chaining involves breaking complex tasks into multiple prompts. Prompt engineering requires understanding the model's capabilities and limitations, as well as iterative testing and refinement. While prompt engineering can significantly improve results, it's not a substitute for fine-tuning when domain-specific knowledge is required. The field continues to evolve with new techniques and best practices emerging as models improve. Effective prompt engineering can make the difference between useful and unusable outputs from language models.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["ai"] * len(memories),
        sources=["research"] * len(memories)
    )


def create_database_memories(service: RagMemoryService):
    """Create memories about databases."""
    logger.info("Creating database memories...")
    
    memories = [
        "PostgreSQL is a powerful open-source relational database with ACID compliance. Developed at the University of California, Berkeley, PostgreSQL has been actively developed for over 30 years. It supports both SQL (relational) and JSON (non-relational) querying, making it versatile for various use cases. PostgreSQL provides advanced features like full-text search, array data types, JSON/JSONB support, and extensibility through custom functions and data types. The database supports complex queries, transactions, foreign keys, triggers, and views. PostgreSQL's MVCC (Multi-Version Concurrency Control) allows multiple transactions to occur simultaneously without blocking. It has excellent performance for read-heavy workloads and can handle complex analytical queries. The database supports various indexing strategies including B-tree, hash, GiST, SP-GiST, GIN, and BRIN indexes. PostgreSQL is known for its standards compliance and extensibility - developers can add custom data types, operators, and functions. It runs on all major operating systems and supports replication, high availability, and backup solutions. PostgreSQL's active community and regular updates ensure it stays current with modern database needs. Many organizations choose PostgreSQL for its reliability, feature richness, and zero licensing costs.",
        "MySQL is a popular open-source relational database management system. Originally developed by MySQL AB and now owned by Oracle Corporation, MySQL is one of the most widely used databases in the world. It's particularly popular for web applications and is the database of choice for many content management systems like WordPress, Drupal, and Joomla. MySQL is known for its speed, reliability, and ease of use. It supports multiple storage engines, with InnoDB being the default and providing ACID compliance and foreign key support. MySQL's replication capabilities allow for read scaling and high availability setups. The database supports both SQL and NoSQL interfaces, with JSON data type support added in recent versions. MySQL has a large ecosystem of tools, libraries, and frameworks that support it. It's optimized for read-heavy workloads and can handle millions of queries per second on appropriate hardware. MySQL's partitioning feature allows large tables to be split across multiple physical files. The database includes features like stored procedures, triggers, views, and full-text search. MySQL's popularity means there's extensive documentation, community support, and third-party tools available. While it's free and open-source, Oracle also offers commercial versions with additional features and support.",
        "MongoDB is a document-oriented NoSQL database for flexible schemas. Instead of storing data in tables with rows and columns like relational databases, MongoDB stores data in flexible, JSON-like documents. This schema flexibility makes it easy to evolve data structures as application requirements change. MongoDB documents are stored in collections, which are analogous to tables in relational databases. The database supports rich queries including range queries, regular expressions, and geospatial queries. MongoDB's document model maps naturally to objects in application code, reducing the need for object-relational mapping. The database supports horizontal scaling through sharding, automatically distributing data across multiple servers. MongoDB's replica sets provide high availability and automatic failover. The database includes features like indexing, aggregation pipelines, and change streams for real-time data processing. MongoDB Atlas is the cloud-managed version, providing automated backups, monitoring, and scaling. The database is particularly well-suited for content management, real-time analytics, mobile applications, and IoT applications. MongoDB's flexible schema allows for rapid development and iteration. However, the lack of joins and transactions (in early versions) requires careful data modeling. Modern MongoDB versions support multi-document ACID transactions, bringing it closer to relational database capabilities while maintaining schema flexibility.",
        "Redis is an in-memory data structure store used for caching. Created by Salvatore Sanfilippo, Redis stands for Remote Dictionary Server and is often described as a data structure server. While it can persist data to disk, Redis primarily stores data in memory, making it extremely fast for read and write operations. Redis supports various data structures including strings, hashes, lists, sets, sorted sets, bitmaps, hyperloglogs, and streams. This versatility makes Redis useful for many use cases beyond simple caching, including session storage, message queues, real-time analytics, and leaderboards. Redis's pub/sub messaging system enables real-time communication between applications. The database supports atomic operations, transactions, and Lua scripting for complex operations. Redis can be configured with different persistence options: RDB snapshots, AOF (Append Only File), or both. Redis Cluster provides automatic sharding and high availability across multiple nodes. The database is single-threaded, which simplifies its design and ensures atomic operations. Redis's performance makes it ideal for use cases requiring sub-millisecond latency. Common patterns include caching database queries, storing session data, implementing rate limiting, and managing distributed locks. Redis is often used as a caching layer in front of slower databases, significantly improving application performance. The database has a simple command-line interface and supports many programming languages through client libraries.",
        "Elasticsearch is a distributed search and analytics engine. Built on Apache Lucene, Elasticsearch provides a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. It's commonly used for log analytics, full-text search, business intelligence, and operational intelligence. Elasticsearch stores data as JSON documents and provides a powerful query language for searching and analyzing that data. The engine is built for horizontal scalability, automatically distributing data and queries across multiple nodes. Elasticsearch uses an inverted index structure, which makes full-text search extremely fast. The engine supports complex queries including fuzzy matching, phrase matching, and aggregations. Elasticsearch is often used with Logstash for data ingestion and Kibana for visualization, forming the ELK stack. The engine supports real-time search, meaning documents become searchable immediately after indexing. Elasticsearch's aggregation framework enables complex analytics including metrics, buckets, and pipeline aggregations. The engine provides features like highlighting, suggestions, and geospatial search. Elasticsearch is schema-free but supports explicit mapping definitions for better control. The engine's distributed nature provides high availability and fault tolerance through replica shards. Elasticsearch is used by companies for search functionality, log analysis, monitoring, and business intelligence. The engine's RESTful API makes it easy to integrate with various applications and programming languages.",
        "SQLite is a lightweight embedded database for local storage. Unlike most SQL databases, SQLite is not a separate server process but is embedded directly into applications. The entire database is stored in a single file, making it easy to deploy and backup. SQLite is self-contained, requiring no external dependencies, and is included in many operating systems and programming languages. Despite its simplicity, SQLite supports most SQL features including transactions, triggers, views, and foreign keys. The database is ACID-compliant and handles concurrency through file locking. SQLite is ideal for applications that need a local database without the overhead of a database server, such as mobile apps, desktop applications, and small web applications. The database can handle databases up to 281 terabytes in size, though it's optimized for smaller databases. SQLite's zero-configuration nature makes it perfect for development, testing, and prototyping. The database is used extensively in browsers, mobile operating systems, and embedded systems. SQLite's reliability and simplicity have made it one of the most widely deployed databases in the world. However, SQLite is not suitable for high-concurrency write scenarios or applications requiring network access to the database. The database's file-based nature means it can't scale across multiple servers, but for many use cases, this simplicity is a feature, not a limitation.",
        "Cassandra is a distributed NoSQL database for high availability. Developed at Facebook and now an Apache project, Cassandra is designed to handle large amounts of data across many commodity servers with no single point of failure. Cassandra's architecture is masterless, meaning all nodes are equal and can handle read and write requests. This design provides high availability and fault tolerance - if a node fails, the system continues operating. Cassandra uses a peer-to-peer distributed system architecture across homogeneous nodes where data is distributed among all nodes in the cluster. The database is optimized for write-heavy workloads and can handle millions of writes per second. Cassandra's data model is based on a wide-column store, organizing data into tables with rows and columns. The database supports eventual consistency, allowing for high availability and partition tolerance as per the CAP theorem. Cassandra's tunable consistency allows developers to balance consistency and availability based on their needs. The database supports CQL (Cassandra Query Language), which is similar to SQL but adapted for Cassandra's distributed nature. Cassandra is particularly well-suited for time-series data, IoT applications, and applications requiring high write throughput. The database's linear scalability means you can add nodes to increase capacity. Cassandra is used by companies like Netflix, Apple, and Instagram for handling massive amounts of data with high availability requirements.",
        "DynamoDB is AWS's managed NoSQL database service. As a fully managed service, DynamoDB handles hardware provisioning, setup, configuration, replication, software patching, and cluster scaling automatically. This allows developers to focus on application development rather than database management. DynamoDB provides single-digit millisecond performance at any scale, with built-in security, backup and restore, and in-memory caching. The service supports both document and key-value data models, with flexible schema design. DynamoDB tables can scale automatically to handle traffic spikes, and you only pay for the resources you use. The service provides two capacity modes: on-demand, which automatically scales, and provisioned, which allows you to specify capacity. DynamoDB Streams enables real-time processing of data changes, useful for building event-driven applications. The service supports global tables for multi-region, multi-active replication. DynamoDB Accelerator (DAX) provides in-memory caching for microsecond latency. The service integrates seamlessly with other AWS services like Lambda, API Gateway, and S3. DynamoDB's point-in-time recovery allows you to restore tables to any point in time within the last 35 days. The service supports transactions, enabling atomic operations across multiple items. DynamoDB is ideal for applications requiring consistent, single-digit millisecond latency at any scale. The service's serverless nature and automatic scaling make it attractive for applications with unpredictable workloads.",
        "Neo4j is a graph database for connected data relationships. Unlike relational databases that use tables and rows, Neo4j stores data as nodes and relationships, making it ideal for representing and querying connected data. The database uses a property graph model where nodes can have properties and labels, and relationships can also have properties and types. Neo4j's Cypher query language is designed specifically for working with graph data, making it intuitive to express graph patterns. The database excels at traversing relationships, making it fast for queries that involve multiple hops through connected data. Neo4j is particularly well-suited for use cases like social networks, recommendation engines, fraud detection, knowledge graphs, and network analysis. The database can handle complex queries that would require expensive joins in relational databases. Neo4j provides ACID transactions and supports both read and write operations with consistency guarantees. The database includes features like full-text search, spatial data types, and graph algorithms. Neo4j's graph data science library provides pre-built algorithms for common graph analytics tasks. The database can be deployed as a standalone server or in cluster mode for high availability and scalability. Neo4j's visualization tools help developers understand and explore their graph data. The database is used by companies for applications ranging from real-time recommendations to master data management. Neo4j's ability to model and query complex relationships makes it powerful for applications where connections between data are as important as the data itself.",
        "Vector databases like pgvector enable semantic search. Traditional databases are optimized for exact matches and range queries, but vector databases are designed for similarity search using embeddings. pgvector is a PostgreSQL extension that adds vector data type and similarity search capabilities to PostgreSQL. This allows you to store embeddings alongside traditional relational data and perform fast similarity searches. Vector databases use specialized indexing algorithms like HNSW (Hierarchical Navigable Small World) to enable fast approximate nearest neighbor search even with millions of vectors. The similarity metric, typically cosine similarity or Euclidean distance, determines how vectors are compared. Vector databases are essential for RAG (Retrieval-Augmented Generation) systems, where relevant documents are retrieved based on semantic similarity to queries. They enable applications like semantic search, recommendation systems, and similarity matching. pgvector integrates vector search with PostgreSQL's relational capabilities, allowing you to combine semantic search with traditional SQL queries and filters. Other vector databases like Pinecone, Weaviate, and Milvus are purpose-built for vector operations and may offer better performance for pure vector workloads. Vector databases typically support operations like inserting vectors, querying for similar vectors, and filtering by metadata. They're crucial for AI applications that need to find semantically similar content quickly. The ability to search by meaning rather than exact keywords opens up new possibilities for information retrieval and discovery.",
        "InfluxDB is a time-series database optimized for handling time-stamped data. Designed specifically for metrics, events, and other time-series data, InfluxDB is built to handle high write and query loads. The database uses a schema-less design that's optimized for time-series data, with tags for indexed metadata and fields for values. InfluxDB's query language, InfluxQL, is SQL-like but designed for time-series operations. The database excels at storing and querying data points that are associated with timestamps, such as sensor readings, application metrics, and IoT data. InfluxDB automatically handles data retention policies, downsampling, and compression to manage storage efficiently. The database supports continuous queries for real-time data aggregation and alerting. InfluxDB's TSM (Time-Structured Merge Tree) storage engine is optimized for time-series workloads. The database can handle millions of data points per second and provides fast queries for time-range and aggregation operations. InfluxDB is commonly used for monitoring, IoT applications, real-time analytics, and DevOps metrics. The database integrates well with visualization tools like Grafana for creating dashboards. InfluxDB's ability to handle high-cardinality data makes it suitable for complex monitoring scenarios. The database supports both open-source and cloud-managed versions, providing flexibility for different deployment needs.",
        "CouchDB is a document-oriented NoSQL database with a focus on ease of use and scalability. CouchDB stores data as JSON documents and provides a RESTful HTTP API for all operations. The database uses Multi-Version Concurrency Control (MVCC) to handle concurrent access without locking. CouchDB's replication is built-in and bidirectional, making it easy to sync data between servers, mobile devices, and browsers. The database includes a built-in web interface (Futon) for administration and data browsing. CouchDB uses MapReduce views for querying and aggregating data, which are defined using JavaScript functions. The database's append-only storage model provides durability and allows for efficient replication. CouchDB supports eventual consistency, making it suitable for distributed systems where network partitions can occur. The database's document model allows for flexible schemas, making it easy to evolve data structures. CouchDB is particularly well-suited for applications requiring offline capabilities and synchronization, such as mobile apps and distributed systems. The database's conflict resolution mechanism handles cases where the same document is modified in multiple locations. CouchDB's simplicity and ease of use make it attractive for rapid prototyping and development. The database is used in various applications including content management, mobile applications, and distributed systems where replication and synchronization are important.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["databases"] * len(memories),
        sources=["documentation"] * len(memories)
    )


def create_web_development_memories(service: RagMemoryService):
    """Create memories about web development."""
    logger.info("Creating web development memories...")
    
    memories = [
        "React is a JavaScript library for building user interfaces, developed by Facebook. React uses a component-based architecture where UI is broken down into reusable, composable components. The library introduced the virtual DOM concept, which creates an in-memory representation of the DOM and efficiently updates only the parts that changed. React's declarative approach makes code more predictable and easier to debug. The library supports JSX, a syntax extension that allows writing HTML-like code in JavaScript. React's one-way data flow and state management make applications easier to reason about. React Hooks, introduced in version 16.8, allow functional components to use state and lifecycle methods. The library has a large ecosystem including React Router for navigation, Redux for state management, and Next.js for full-stack applications. React's component reusability reduces code duplication and improves maintainability. The library's performance optimizations like memoization and code splitting help build fast applications. React Native extends React to mobile development, allowing code sharing between web and mobile. The library's popularity has made it a standard choice for modern web development, with extensive community support and resources available.",
        "Vue.js is a progressive framework for building web applications, created by Evan You. Vue is designed to be incrementally adoptable, meaning you can use as much or as little of it as needed. The framework combines the best aspects of React and Angular, offering a simple learning curve with powerful features. Vue uses a template-based syntax that's easy to understand for developers familiar with HTML. The framework's reactivity system automatically tracks dependencies and updates the DOM when data changes. Vue's component system allows for building complex applications from simple, reusable components. The framework includes built-in directives like v-if, v-for, and v-bind that extend HTML with additional functionality. Vue's single-file components combine template, script, and style in one file, improving organization. The framework has excellent documentation and a gentle learning curve, making it accessible to beginners. Vue's performance is competitive with React, using a virtual DOM and efficient rendering algorithms. The framework supports both template-based and render function approaches. Vue's ecosystem includes Vue Router for routing, Vuex for state management, and Nuxt.js for server-side rendering. Vue's progressive nature means you can start with a simple script tag and gradually adopt more advanced features as needed.",
        "Angular is a comprehensive framework by Google for web apps, providing a complete solution for building large-scale applications. Angular uses TypeScript as its primary language, providing type safety and better tooling. The framework follows a component-based architecture with a clear separation of concerns. Angular's dependency injection system makes code more modular and testable. The framework includes a powerful CLI tool that generates code, runs tests, and builds applications. Angular's two-way data binding synchronizes the model and view automatically. The framework uses decorators and metadata to configure components, services, and modules. Angular's routing system supports lazy loading, guards, and nested routes for complex navigation. The framework includes built-in support for forms with validation, HTTP client for API calls, and animations. Angular's change detection system efficiently updates the DOM when data changes. The framework's modular architecture allows for code organization and reusability. Angular Material provides a comprehensive set of UI components following Material Design principles. The framework's strong typing and comprehensive tooling make it suitable for enterprise applications. Angular's learning curve is steeper than Vue or React, but it provides more structure and features out of the box.",
        "Next.js is a React framework with server-side rendering and static site generation. Created by Vercel, Next.js extends React with features like file-based routing, API routes, and automatic code splitting. The framework supports both server-side rendering (SSR) and static site generation (SSG), allowing you to choose the best approach for each page. Next.js's Image component automatically optimizes images for performance. The framework's API routes allow you to build backend functionality within the same project. Next.js supports incremental static regeneration, updating static pages without rebuilding the entire site. The framework includes built-in CSS support, including CSS Modules and Sass. Next.js's automatic code splitting ensures users only download the JavaScript they need. The framework supports middleware for running code before requests are completed. Next.js's App Router (in version 13+) provides a new routing system with layouts, loading states, and error boundaries. The framework optimizes performance with features like automatic font optimization and script optimization. Next.js's development experience includes hot module replacement and fast refresh. The framework is widely used for production applications, with excellent performance and SEO capabilities. Next.js's deployment options include Vercel, which provides seamless integration and global CDN distribution.",
        "Svelte compiles components to efficient vanilla JavaScript, resulting in smaller bundle sizes and faster runtime performance. Unlike React and Vue, which include a runtime library, Svelte moves work from runtime to compile time. The framework compiles components into optimized JavaScript that directly manipulates the DOM. Svelte's reactivity is built into the language, using a compiler to track dependencies and update the DOM efficiently. The framework's syntax is simple and intuitive, with minimal boilerplate code. Svelte's component structure combines markup, script, and styles in a single file. The framework includes built-in state management, eliminating the need for external libraries like Redux. Svelte's transitions and animations make it easy to create smooth user experiences. The framework supports stores for managing global state across components. SvelteKit extends Svelte with features like routing, server-side rendering, and static site generation. The framework's small bundle sizes make it ideal for performance-critical applications. Svelte's compiler optimizations result in code that's often faster than hand-written JavaScript. The framework's learning curve is gentle, with concepts that are easy to understand. Svelte's approach of compiling away the framework results in applications that feel like vanilla JavaScript but with the developer experience of a modern framework.",
        "Tailwind CSS is a utility-first CSS framework that provides low-level utility classes to build custom designs. Unlike traditional CSS frameworks that provide pre-built components, Tailwind gives you utility classes that you compose to build your design. The framework's utility classes cover spacing, typography, colors, flexbox, grid, and more. Tailwind's approach allows for rapid UI development without writing custom CSS. The framework uses PurgeCSS to remove unused styles in production, resulting in small CSS files. Tailwind's configuration file allows you to customize colors, spacing, fonts, and other design tokens. The framework supports responsive design with breakpoint prefixes like sm:, md:, and lg:. Tailwind's dark mode support allows you to create themes that adapt to user preferences. The framework's JIT (Just-In-Time) mode generates styles on-demand, improving development experience. Tailwind's plugin system allows you to extend the framework with custom utilities. The framework works well with component-based frameworks, allowing you to build reusable UI components. Tailwind's utility classes are composable, making it easy to create consistent designs. The framework's approach encourages consistency and reduces the need for naming conventions. Tailwind has become popular for its developer experience and ability to build custom designs quickly without leaving HTML.",
        "GraphQL is a query language for APIs with precise data fetching, developed by Facebook. Unlike REST APIs that return fixed data structures, GraphQL allows clients to specify exactly what data they need. This reduces over-fetching and under-fetching of data, improving performance and reducing bandwidth. GraphQL uses a single endpoint for all queries, simplifying API architecture. The query language is strongly typed, with a schema that defines available data and operations. GraphQL supports queries for reading data, mutations for modifying data, and subscriptions for real-time updates. The language's introspection capabilities allow clients to discover the API schema automatically. GraphQL resolvers handle the logic for fetching data from various sources. The language supports nested queries, allowing clients to fetch related data in a single request. GraphQL's type system includes scalars, objects, interfaces, unions, and enums. The language's validation ensures queries are valid before execution. GraphQL has implementations in many programming languages, making it language-agnostic. Popular tools like Apollo and Relay provide client libraries and server implementations. GraphQL's flexibility makes it suitable for complex applications with diverse data requirements. The language's single endpoint and precise queries make it ideal for mobile applications where bandwidth is a concern.",
        "REST APIs use HTTP methods for CRUD operations, providing a standard way to interact with web services. REST stands for Representational State Transfer and follows principles like statelessness, uniform interface, and resource-based URLs. REST APIs use HTTP verbs like GET for reading, POST for creating, PUT for updating, and DELETE for deleting resources. The architecture uses standard HTTP status codes to indicate success or failure. REST APIs typically return data in JSON format, though XML and other formats are also supported. The architecture's stateless nature means each request contains all information needed to process it. REST APIs use resource-based URLs that represent entities, like /users/123 for a specific user. The architecture supports caching through HTTP cache headers, improving performance. REST APIs are language and platform agnostic, working with any client that can make HTTP requests. The architecture's simplicity and familiarity make it easy to understand and implement. REST APIs support content negotiation, allowing clients to request different data formats. The architecture's uniform interface makes APIs predictable and easy to use. REST has become the standard for web APIs, with extensive tooling and documentation available. The architecture's maturity and widespread adoption make it a safe choice for API design. REST APIs work well with HTTP/2 and can be enhanced with features like pagination, filtering, and sorting.",
        "WebSockets enable real-time bidirectional communication between clients and servers. Unlike HTTP, which follows a request-response pattern, WebSockets maintain a persistent connection that allows both client and server to send messages at any time. This makes WebSockets ideal for applications requiring real-time updates, such as chat applications, live notifications, and collaborative editing. The WebSocket protocol starts with an HTTP handshake and then upgrades to a full-duplex communication channel. WebSockets reduce latency by eliminating the overhead of establishing new connections for each message. The protocol supports both text and binary data transmission. WebSocket connections can be secured using WSS (WebSocket Secure), similar to HTTPS. The protocol's event-driven nature makes it efficient for applications with frequent, small messages. WebSockets are supported by all modern browsers and have server implementations in most programming languages. The protocol's low overhead makes it suitable for high-frequency messaging scenarios. WebSocket libraries often provide features like automatic reconnection, heartbeat messages, and message queuing. The protocol is particularly useful for applications like online gaming, financial trading platforms, and real-time dashboards. WebSockets complement REST APIs, with REST handling standard CRUD operations and WebSockets handling real-time features. The protocol's bidirectional nature enables server-initiated updates, which isn't possible with standard HTTP. WebSockets have become essential for building modern, interactive web applications.",
        "Progressive Web Apps (PWAs) work offline and feel native, combining the best of web and mobile apps. PWAs use service workers to cache resources and enable offline functionality. The apps can be installed on devices, appearing in app launchers and running in standalone windows. PWAs use web app manifests to define how the app appears when installed. The apps work across all platforms, providing a single codebase for web, iOS, and Android. PWAs use HTTPS to ensure secure connections and enable service workers. The apps can send push notifications, engaging users even when the app isn't open. PWAs are discoverable through search engines, unlike native apps in app stores. The apps are linkable, allowing users to share specific content via URLs. PWAs automatically update, ensuring users always have the latest version. The apps are responsive, working on any screen size or device. PWAs can access device features like cameras, geolocation, and file systems through web APIs. The apps provide a native-like experience with smooth animations and transitions. PWAs reduce development and maintenance costs by using a single codebase. The apps have smaller file sizes than native apps, reducing download times. PWAs have become a popular choice for businesses wanting to reach users across platforms with a single application.",
        "Server-Side Rendering (SSR) generates HTML on the server for each request, improving initial page load times and SEO. SSR sends fully rendered HTML to the browser, which can display content immediately while JavaScript loads. This approach improves perceived performance, especially on slower networks. SSR ensures search engines can crawl and index content, which is crucial for SEO. The approach works well with frameworks like Next.js, Nuxt.js, and SvelteKit. SSR can improve Core Web Vitals metrics like First Contentful Paint and Largest Contentful Paint. The approach requires server resources to render pages, which can increase server costs. SSR can improve security by keeping sensitive logic on the server. The approach supports dynamic content that changes based on user context or request parameters. SSR works well for content-heavy sites where SEO is important. The approach can be combined with static site generation for optimal performance. SSR's initial HTML includes all content, making it accessible even if JavaScript fails to load. The approach requires careful consideration of what to render on the server versus the client. SSR has become a standard approach for modern web applications, balancing performance, SEO, and user experience.",
        "Web Components are a set of web platform APIs that allow you to create reusable custom HTML elements. The technology consists of three main features: Custom Elements, Shadow DOM, and HTML Templates. Custom Elements allow you to define new HTML tags with custom behavior. Shadow DOM provides encapsulation, keeping styles and markup separate from the rest of the page. HTML Templates define reusable markup that isn't rendered until activated. Web Components work with any framework or library, making them framework-agnostic. The technology uses standard web APIs, ensuring compatibility across browsers. Web Components can be used to create design systems that work across different frameworks. The technology's encapsulation prevents style conflicts and DOM pollution. Web Components are particularly useful for building reusable UI libraries. The technology supports lifecycle callbacks like connectedCallback and disconnectedCallback. Web Components can communicate with the outside world through events and properties. The technology's standardization ensures long-term compatibility and support. Web Components work well with modern build tools and can be distributed via npm. The technology enables true component reusability across different projects and frameworks. Web Components have gained adoption as browsers have improved support and tooling has matured.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["webdev"] * len(memories),
        sources=["documentation"] * len(memories)
    )


def create_devops_memories(service: RagMemoryService):
    """Create memories about DevOps and infrastructure."""
    logger.info("Creating DevOps memories...")
    
    memories = [
        "Docker containers package applications with their dependencies, providing consistent environments across development, testing, and production. Containers isolate applications from the underlying system, ensuring they run the same way regardless of where they're deployed. Docker uses containerization technology to package an application and its dependencies into a lightweight, portable image. Containers share the host system's kernel but have isolated filesystems, networks, and processes. Docker images are built from Dockerfiles, which define the steps to create a containerized application. The technology enables developers to build once and run anywhere, reducing 'it works on my machine' problems. Docker containers start quickly and use fewer resources than virtual machines. The Docker Hub provides a registry of pre-built images that can be used as base images. Docker Compose allows you to define and run multi-container applications with a single command. Containers can be orchestrated using tools like Kubernetes or Docker Swarm for managing containerized applications at scale. Docker's layered filesystem allows for efficient image storage and sharing. The technology has revolutionized software deployment, making it easier to package and distribute applications. Docker containers are particularly useful for microservices architectures, where each service runs in its own container. The technology's portability and consistency make it essential for modern DevOps practices.",
        "Kubernetes orchestrates containerized applications at scale, automating deployment, scaling, and management. Originally developed by Google, Kubernetes provides a platform for running containerized workloads across clusters of machines. The system abstracts away the underlying infrastructure, allowing you to focus on application deployment. Kubernetes uses a declarative model where you describe the desired state, and the system works to maintain that state. The platform supports automatic scaling based on CPU, memory, or custom metrics. Kubernetes provides self-healing capabilities, automatically restarting failed containers and replacing unhealthy nodes. The system includes service discovery and load balancing, automatically distributing traffic across healthy pods. Kubernetes supports rolling updates, allowing you to update applications without downtime. The platform's resource management ensures containers get the CPU and memory they need. Kubernetes supports persistent storage through volumes and persistent volume claims. The system's namespace feature allows you to organize and isolate resources within a cluster. Kubernetes has a rich ecosystem of extensions and tools for monitoring, logging, and security. The platform supports both stateless and stateful applications, with special handling for databases and other stateful services. Kubernetes has become the standard for container orchestration, with support from all major cloud providers. The platform's flexibility and power make it essential for running containerized applications in production.",
        "CI/CD pipelines automate testing and deployment processes, enabling rapid and reliable software delivery. Continuous Integration (CI) automatically builds and tests code changes, catching issues early in the development process. Continuous Deployment (CD) automatically deploys code that passes tests to production environments. CI/CD pipelines typically include stages for building, testing, security scanning, and deployment. The pipelines run automatically on code commits, pull requests, or scheduled intervals. CI/CD reduces manual errors and speeds up the feedback loop for developers. The pipelines can be configured to run different tests at different stages, from unit tests to integration tests. CI/CD tools integrate with version control systems, automatically triggering builds on code changes. The pipelines support parallel execution, running multiple tests simultaneously to reduce build times. CI/CD enables feature flags and canary deployments, allowing gradual rollouts of new features. The pipelines can deploy to multiple environments, from development to staging to production. CI/CD tools provide visibility into build status, test results, and deployment history. The pipelines support rollback capabilities, allowing quick reversion to previous versions if issues occur. CI/CD has become essential for modern software development, enabling teams to release software frequently and reliably. The practice reduces the risk of deployments and increases confidence in code changes.",
        "GitHub Actions provides CI/CD integrated with Git repositories, enabling automation directly in your development workflow. Actions are defined in YAML files stored in the repository, making them version-controlled and transparent. The platform supports workflows that trigger on events like pushes, pull requests, and scheduled times. GitHub Actions provides a marketplace of pre-built actions that can be reused across projects. The platform supports matrix builds, allowing you to test across multiple operating systems and language versions. Actions can be triggered by external events through webhooks, enabling integration with other tools. The platform provides secrets management for storing sensitive information like API keys. GitHub Actions supports conditional execution, allowing workflows to run based on file changes or other conditions. The platform includes built-in support for Docker containers, enabling consistent build environments. Actions can be shared across repositories using reusable workflows. The platform provides detailed logs and artifacts, making it easy to debug failed builds. GitHub Actions supports deployment to various platforms, including cloud providers and container registries. The platform's integration with GitHub makes it seamless to set up CI/CD for projects. GitHub Actions has become a popular choice for CI/CD due to its tight integration with the development workflow and generous free tier for public repositories.",
        "Terraform enables infrastructure as code for cloud resources, allowing you to define and manage infrastructure using declarative configuration files. Created by HashiCorp, Terraform uses a domain-specific language (HCL) to describe infrastructure resources. The tool supports multiple cloud providers, including AWS, Azure, GCP, and many others. Terraform maintains state files that track the current state of your infrastructure. The tool's plan command shows what changes will be made before applying them, providing safety and visibility. Terraform supports modules, allowing you to create reusable infrastructure components. The tool's provider system allows you to manage resources across different services and platforms. Terraform can manage both cloud resources and on-premises infrastructure. The tool supports variables and outputs, making configurations flexible and reusable. Terraform's dependency graph ensures resources are created in the correct order. The tool supports workspaces, allowing you to manage multiple environments with the same configuration. Terraform can import existing infrastructure, making it possible to adopt infrastructure as code gradually. The tool's state locking prevents concurrent modifications that could corrupt infrastructure. Terraform has become the standard for infrastructure as code, with extensive documentation and community support. The tool's declarative approach and multi-cloud support make it essential for modern infrastructure management.",
        "Ansible automates configuration management and deployment, using simple YAML playbooks to describe system configurations. Created by Red Hat, Ansible is agentless, meaning it doesn't require software to be installed on managed nodes. The tool uses SSH to connect to remote systems and execute tasks. Ansible's idempotent nature means running the same playbook multiple times produces the same result. The tool includes a large collection of modules for managing various systems and services. Ansible playbooks are human-readable YAML files that describe the desired state of systems. The tool supports roles, allowing you to organize and reuse configuration tasks. Ansible Vault encrypts sensitive data like passwords and API keys. The tool can manage both Linux and Windows systems, as well as network devices and cloud resources. Ansible's inventory system allows you to organize and group hosts for management. The tool supports variables and templates, making playbooks flexible and reusable. Ansible Tower (now AWX) provides a web interface and API for managing Ansible at scale. The tool's simplicity and agentless architecture make it easy to get started and maintain. Ansible has become popular for configuration management, application deployment, and orchestration tasks. The tool's declarative approach and extensive module library make it powerful yet accessible.",
        "Prometheus monitors systems and collects metrics, providing a time-series database and query language for analyzing system performance. Created at SoundCloud, Prometheus is designed for reliability and can run independently of the systems it monitors. The tool uses a pull model, scraping metrics from instrumented applications at regular intervals. Prometheus stores metrics as time-series data, identified by metric names and key-value pairs called labels. The tool's PromQL query language allows you to select and aggregate time-series data. Prometheus includes an alerting system that can trigger notifications based on metric values. The tool supports service discovery, automatically finding and monitoring new services. Prometheus exporters allow you to collect metrics from systems that don't natively support Prometheus. The tool's data model is simple yet powerful, making it easy to instrument applications. Prometheus supports recording rules, allowing you to pre-compute frequently used queries. The tool can handle millions of time series and is designed for reliability and performance. Prometheus is often used with Grafana for visualization, though it includes its own expression browser. The tool has become the standard for metrics collection in cloud-native environments. Prometheus's pull model and simple data model make it easy to set up and maintain. The tool's reliability and performance make it essential for monitoring production systems.",
        "Grafana visualizes metrics and creates dashboards, providing a powerful platform for observability and monitoring. The tool connects to various data sources including Prometheus, InfluxDB, Elasticsearch, and many others. Grafana's dashboard system allows you to create custom visualizations using panels like graphs, tables, and heatmaps. The tool supports alerting, sending notifications when metrics cross thresholds. Grafana's query editor makes it easy to explore and visualize time-series data. The tool supports templating, allowing you to create dynamic dashboards that work across different environments. Grafana includes user authentication and authorization, supporting multiple authentication providers. The tool's plugin system allows you to extend functionality with custom data sources and panels. Grafana supports annotations, marking events on graphs for context. The tool's explore feature allows you to ad-hoc query data sources without creating dashboards. Grafana can be deployed on-premises or in the cloud, with options for high availability. The tool's beautiful visualizations and ease of use make it popular for monitoring and observability. Grafana has become the standard tool for visualizing metrics, often used alongside Prometheus. The tool's flexibility and extensive data source support make it suitable for various monitoring scenarios. Grafana's alerting and notification features help teams respond quickly to system issues.",
        "Load balancers distribute traffic across multiple servers, ensuring high availability and optimal resource utilization. Load balancers act as a reverse proxy, receiving requests and forwarding them to backend servers. The technology distributes traffic using algorithms like round-robin, least connections, or IP hash. Load balancers can perform health checks, automatically removing unhealthy servers from the pool. The technology supports SSL termination, handling encryption and decryption to reduce backend server load. Load balancers can perform content-based routing, directing requests to specific servers based on URL or headers. The technology supports session persistence, ensuring users are directed to the same server when needed. Load balancers can be hardware-based, software-based, or cloud-managed services. The technology provides high availability by automatically routing traffic away from failed servers. Load balancers can perform compression and caching to improve performance. The technology supports geographic distribution, routing users to the nearest data center. Load balancers are essential for scaling applications horizontally, adding more servers as traffic increases. The technology provides visibility into traffic patterns and server health. Load balancers have become essential infrastructure components for modern applications. The technology's ability to distribute traffic and handle failures makes it critical for high-availability systems.",
        "Microservices architecture splits applications into small services, each running independently and communicating over networks. Each microservice is responsible for a specific business capability and can be developed, deployed, and scaled independently. The architecture allows teams to use different technologies and programming languages for different services. Microservices enable faster development cycles, as teams can work on services independently. The architecture improves fault isolation, as failures in one service don't necessarily affect others. Microservices can be scaled independently based on demand, optimizing resource usage. The architecture requires service discovery mechanisms to locate and communicate with services. Microservices typically use APIs for communication, often REST or message queues. The architecture introduces complexity in areas like distributed transactions and data consistency. Microservices require robust monitoring and logging to understand system behavior across services. The architecture works well with containerization and orchestration tools like Docker and Kubernetes. Microservices enable organizations to adopt new technologies gradually, service by service. The architecture requires careful design of service boundaries and interfaces. Microservices have become popular for large-scale applications where flexibility and scalability are important. The architecture's benefits come with operational complexity that requires mature DevOps practices.",
        "Infrastructure as Code (IaC) manages infrastructure through code rather than manual processes, bringing software engineering practices to infrastructure management. IaC allows you to version control infrastructure configurations, enabling collaboration and change tracking. The practice makes infrastructure reproducible, allowing you to recreate environments consistently. IaC enables automated testing of infrastructure changes before applying them. The practice reduces human error by eliminating manual configuration steps. IaC tools like Terraform, CloudFormation, and Pulumi allow you to define infrastructure declaratively. The practice supports multiple environments, allowing you to manage dev, staging, and production with the same code. IaC enables infrastructure reviews through code review processes, improving quality and security. The practice makes disaster recovery easier, as infrastructure can be recreated from code. IaC supports documentation, as code serves as living documentation of infrastructure. The practice enables infrastructure testing, allowing you to validate configurations before deployment. IaC has become essential for cloud-native applications, where infrastructure changes frequently. The practice's benefits include consistency, speed, and reliability in infrastructure management. IaC represents a fundamental shift in how infrastructure is managed, bringing agility and reliability to operations.",
        "Container orchestration manages containerized applications across clusters of machines, handling deployment, scaling, and operations. Orchestration platforms like Kubernetes, Docker Swarm, and Nomad automate container lifecycle management. These platforms handle scheduling, determining which nodes should run which containers. Orchestration provides service discovery, allowing containers to find and communicate with each other. The platforms support load balancing, distributing traffic across multiple container instances. Orchestration handles health checks and automatic restarts, ensuring application availability. The platforms support rolling updates, allowing you to update applications without downtime. Orchestration provides resource management, ensuring containers get the CPU and memory they need. The platforms support persistent storage, allowing stateful applications to store data. Orchestration enables horizontal scaling, automatically adding or removing containers based on demand. The platforms provide networking abstractions, simplifying communication between containers. Orchestration has become essential for running containerized applications in production. The platforms' automation reduces operational overhead and improves reliability. Container orchestration represents a key technology for modern cloud-native applications, enabling teams to manage complex distributed systems effectively.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["devops"] * len(memories),
        sources=["documentation"] * len(memories)
    )


def create_data_science_memories(service: RagMemoryService):
    """Create memories about data science."""
    logger.info("Creating data science memories...")
    
    memories = [
        "Pandas is a Python library for data manipulation and analysis, providing data structures and operations for working with structured data. The library's primary data structures are Series (one-dimensional) and DataFrame (two-dimensional), which are built on NumPy arrays. Pandas provides powerful tools for reading and writing data in various formats including CSV, Excel, JSON, SQL, and Parquet. The library includes functions for data cleaning, including handling missing values, removing duplicates, and data type conversion. Pandas supports advanced indexing and selection, allowing you to filter and slice data efficiently. The library provides groupby operations for aggregating and transforming data by groups. Pandas includes time series functionality, making it ideal for working with temporal data. The library supports merging and joining operations, similar to SQL joins. Pandas provides vectorized operations that are much faster than Python loops. The library's integration with NumPy, Matplotlib, and other scientific libraries makes it a cornerstone of the Python data science ecosystem. Pandas is widely used for data exploration, cleaning, transformation, and analysis. The library's intuitive API and comprehensive documentation make it accessible to beginners. Pandas has become the standard tool for data manipulation in Python, used by data scientists, analysts, and engineers worldwide.",
        "NumPy provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. The library is the foundation of the Python scientific computing ecosystem, with many other libraries built on top of it. NumPy arrays are more efficient than Python lists for numerical computations, as they're implemented in C. The library provides vectorized operations that apply functions to entire arrays without explicit loops. NumPy's broadcasting feature allows operations between arrays of different shapes. The library includes functions for linear algebra, Fourier transforms, and random number generation. NumPy arrays support various data types and can be efficiently converted between types. The library provides tools for array manipulation, including reshaping, slicing, and concatenation. NumPy's universal functions (ufuncs) operate element-wise on arrays, providing fast mathematical operations. The library integrates well with C and Fortran code, allowing performance-critical sections to be optimized. NumPy's memory-efficient design makes it suitable for working with large datasets. The library is essential for scientific computing, machine learning, and data analysis in Python. NumPy's array operations are the building blocks for higher-level libraries like Pandas and Scikit-learn. The library's performance and functionality make it indispensable for numerical computing in Python.",
        "Matplotlib creates static, animated, and interactive visualizations, providing a comprehensive plotting library for Python. The library's pyplot interface provides a MATLAB-like plotting experience, making it easy to create common visualizations. Matplotlib supports a wide variety of plot types including line plots, scatter plots, bar charts, histograms, and heatmaps. The library provides fine-grained control over plot appearance, including colors, labels, legends, and annotations. Matplotlib can save figures in various formats including PNG, PDF, SVG, and EPS. The library supports subplots, allowing you to create multi-panel figures. Matplotlib's object-oriented API provides more control for complex visualizations. The library integrates well with NumPy and Pandas, making it easy to plot data from these sources. Matplotlib supports 3D plotting through its mplot3d toolkit. The library's style system allows you to customize the appearance of plots globally. Matplotlib is the foundation for many other visualization libraries, which build on its capabilities. The library is widely used in scientific publications, data analysis, and exploratory data science. Matplotlib's flexibility and comprehensive feature set make it the standard for plotting in Python. The library's extensive documentation and examples make it accessible to users of all levels.",
        "Scikit-learn offers machine learning algorithms for Python, providing a unified interface for various machine learning tasks. The library includes implementations of classification, regression, clustering, and dimensionality reduction algorithms. Scikit-learn's consistent API makes it easy to try different algorithms and compare their performance. The library provides tools for data preprocessing, including scaling, encoding, and feature selection. Scikit-learn includes model evaluation tools like cross-validation, metrics, and learning curves. The library supports pipeline creation, allowing you to chain preprocessing and modeling steps. Scikit-learn provides utilities for model selection, including grid search and random search. The library includes sample datasets for learning and testing algorithms. Scikit-learn's design principles emphasize consistency, inspection, and non-proliferation of classes. The library is built on NumPy, SciPy, and Matplotlib, integrating well with the scientific Python ecosystem. Scikit-learn is widely used in industry and academia for machine learning applications. The library's comprehensive documentation and examples make it accessible to beginners. Scikit-learn's focus on traditional machine learning algorithms makes it complementary to deep learning frameworks. The library's reliability and performance make it a standard tool for machine learning in Python.",
        "Jupyter notebooks enable interactive data analysis, combining code, text, and visualizations in a single document. Notebooks allow you to execute code in cells, making it easy to experiment and iterate on analysis. The interactive nature of notebooks makes them ideal for exploratory data analysis and prototyping. Jupyter supports multiple programming languages through kernels, though Python is most common. Notebooks can include markdown cells for documentation, explanations, and narrative text. The ability to mix code and documentation makes notebooks excellent for sharing analyses and results. Jupyter notebooks support rich output including HTML, images, videos, and interactive widgets. Notebooks can be exported to various formats including HTML, PDF, and slides. JupyterLab provides an enhanced interface with file browsers, terminals, and multiple notebook tabs. Notebooks support extensions that add functionality like table of contents, code formatting, and spell checking. Jupyter's interactive widgets allow you to create interactive visualizations and dashboards. Notebooks can be shared and run on cloud platforms like Google Colab and Kaggle. The notebook format has become standard for data science education and collaboration. Jupyter's ability to combine code, results, and narrative makes it powerful for reproducible research. Notebooks have become essential tools for data scientists, enabling exploration, analysis, and communication of results.",
        "Data cleaning prepares raw data for analysis, addressing issues like missing values, duplicates, and inconsistencies. The process is often the most time-consuming part of data science projects, but it's crucial for accurate results. Data cleaning involves identifying and handling missing values through imputation or removal. The process includes detecting and removing duplicate records that could skew analysis. Data cleaning addresses inconsistencies in formats, such as dates, phone numbers, and addresses. The process involves handling outliers that may represent errors or require special treatment. Data cleaning includes type conversion, ensuring data types match their intended use. The process involves standardizing text data, handling case, whitespace, and special characters. Data cleaning includes validation, checking that data meets expected constraints and ranges. The process may involve merging data from multiple sources, requiring careful handling of joins. Data cleaning tools and libraries automate many common cleaning tasks. The process requires domain knowledge to make appropriate decisions about handling data issues. Data cleaning is iterative, often revealing new issues as analysis progresses. Clean data is essential for reliable machine learning models and accurate analysis. The process of data cleaning has been formalized and automated in many tools, but still requires human judgment for complex cases.",
        "Feature engineering creates informative input variables for machine learning models, transforming raw data into features that improve model performance. The process involves selecting, modifying, or creating features from raw data. Feature engineering can include creating interaction terms, combining features to capture relationships. The process involves encoding categorical variables into numerical representations. Feature engineering includes scaling and normalization to ensure features are on similar scales. The process may involve creating polynomial features to capture non-linear relationships. Feature engineering includes handling temporal features, extracting components like day of week or hour of day. The process involves creating aggregate features, summarizing information across groups or time periods. Feature engineering can include dimensionality reduction, creating lower-dimensional representations. The process requires domain knowledge to create features that capture relevant information. Feature engineering is often more important than algorithm choice for model performance. The process is iterative, with features being created, tested, and refined. Automated feature engineering tools can generate many candidate features, but human insight is still valuable. Feature engineering bridges the gap between raw data and what machine learning algorithms can effectively use. The process is a key skill for data scientists, requiring both technical and domain expertise.",
        "Cross-validation evaluates model performance reliably by splitting data into multiple folds and testing on each fold. The technique provides a more robust estimate of model performance than a single train-test split. K-fold cross-validation divides data into k subsets, training on k-1 folds and testing on the remaining fold. The process repeats k times, with each fold serving as the test set once. Cross-validation helps detect overfitting by evaluating performance on multiple data splits. The technique is particularly valuable when you have limited data, as it uses all data for both training and testing. Stratified cross-validation ensures that each fold has the same distribution of target classes. Time series cross-validation uses temporal splits, respecting the time ordering of data. Cross-validation can be used for model selection, comparing different algorithms or hyperparameters. The technique provides confidence intervals for performance metrics, indicating uncertainty. Cross-validation helps identify whether model performance is consistent across different data subsets. The technique is computationally expensive but provides more reliable performance estimates. Cross-validation has become standard practice in machine learning, providing a rigorous way to evaluate models. The technique's ability to use all data for evaluation makes it particularly valuable for small datasets.",
        "A/B testing compares two versions to determine which performs better, using statistical methods to make data-driven decisions. The technique randomly assigns users to different variants and measures outcomes to determine which performs better. A/B tests require careful design, including defining hypotheses, success metrics, and sample sizes. The tests need to run long enough to collect sufficient data for statistical significance. A/B testing helps avoid making decisions based on intuition or small sample sizes. The technique is widely used in web development, marketing, and product development. A/B tests can compare designs, features, algorithms, or any other variation. The technique requires proper randomization to ensure groups are comparable. A/B testing helps quantify the impact of changes, providing concrete evidence for decisions. The tests can reveal unexpected effects, both positive and negative. A/B testing requires statistical knowledge to design tests correctly and interpret results. The technique helps organizations make incremental improvements based on evidence. A/B testing has become standard practice for data-driven decision making. The technique's ability to provide causal evidence makes it powerful for understanding what works. A/B testing represents a scientific approach to optimization, testing hypotheses with real users and data.",
        "Statistical significance determines if results are meaningful, helping distinguish real effects from random variation. Statistical tests calculate p-values, which indicate the probability of observing results as extreme if there's no real effect. A common threshold is p < 0.05, meaning there's less than 5% chance the results are due to chance. However, statistical significance doesn't necessarily mean practical significance - small effects can be statistically significant with large samples. Effect size measures the magnitude of an effect, complementing statistical significance. Multiple testing increases the chance of false positives, requiring corrections like Bonferroni. Statistical power is the probability of detecting an effect when it exists, requiring adequate sample sizes. Confidence intervals provide a range of plausible values for an effect, giving more information than p-values. Statistical significance depends on sample size - larger samples can detect smaller effects. The concept is crucial for interpreting results in experiments, surveys, and A/B tests. Understanding statistical significance helps avoid drawing conclusions from noise. The concept requires careful interpretation, as significance doesn't guarantee importance. Statistical significance is a fundamental concept in data science, helping make evidence-based decisions. The concept's proper use requires understanding its limitations and complementing it with effect sizes and confidence intervals.",
        "Exploratory Data Analysis (EDA) investigates data to discover patterns, anomalies, and relationships before formal modeling. EDA uses visualizations and summary statistics to understand data distributions and characteristics. The process helps identify data quality issues, outliers, and missing values that need attention. EDA reveals relationships between variables, suggesting which features might be important for modeling. The process includes creating histograms, scatter plots, box plots, and correlation matrices. EDA helps formulate hypotheses and guide further analysis and modeling decisions. The process is iterative, with initial findings leading to deeper investigation. EDA includes calculating summary statistics like mean, median, standard deviation, and quartiles. The process helps understand the shape of distributions, identifying skewness and multimodality. EDA can reveal unexpected patterns or anomalies that require investigation. The process helps determine appropriate transformations or preprocessing steps. EDA is often the first step in data science projects, providing foundation for subsequent work. The process combines statistical techniques with visualization to gain insights. EDA's exploratory nature makes it valuable for understanding new datasets. The process helps build intuition about data, which guides modeling and interpretation decisions.",
        "Time series analysis examines data points collected over time to identify patterns, trends, and seasonality. Time series data requires special handling because observations are not independent - values depend on previous values. The analysis includes decomposition, separating trends, seasonal patterns, and random noise. Time series models like ARIMA capture autocorrelation, where values depend on previous values. The analysis helps forecast future values based on historical patterns. Time series require stationarity, where statistical properties don't change over time. The analysis includes identifying and handling seasonality, recurring patterns at regular intervals. Time series models can capture trends, whether increasing, decreasing, or changing direction. The analysis helps understand cyclical patterns that repeat over longer periods. Time series are common in finance, economics, weather, and many other domains. The analysis requires careful handling of missing values and irregular intervals. Time series models can be univariate, using only the series itself, or multivariate, incorporating external factors. The analysis helps make data-driven forecasts for planning and decision-making. Time series analysis is essential for understanding temporal patterns and making predictions. The field combines statistical methods with domain knowledge to extract insights from temporal data.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["datascience"] * len(memories),
        sources=["documentation"] * len(memories)
    )


def create_security_memories(service: RagMemoryService):
    """Create memories about security."""
    logger.info("Creating security memories...")
    
    memories = [
        "Encryption protects data by converting it into unreadable format, ensuring that only authorized parties can access the information. Encryption uses algorithms and keys to transform plaintext into ciphertext that appears random. Symmetric encryption uses the same key for encryption and decryption, making it fast but requiring secure key distribution. Asymmetric encryption uses public and private key pairs, allowing secure communication without sharing secrets. Encryption protects data at rest, stored on disks or databases, preventing unauthorized access if storage is compromised. Encryption also protects data in transit, securing communication over networks. Modern encryption algorithms like AES are considered secure against current computational capabilities. Encryption keys must be managed carefully, as compromised keys allow decryption of protected data. The strength of encryption depends on key length, algorithm choice, and proper implementation. Encryption is essential for protecting sensitive information like personal data, financial records, and communications. The technology enables secure online transactions, protecting credit card numbers and passwords. Encryption has become a fundamental security control, required by regulations and best practices. The technology's proper use requires understanding key management, algorithm selection, and implementation details. Encryption provides confidentiality, ensuring that only intended recipients can read protected data.",
        "HTTPS secures web communication with SSL/TLS certificates, encrypting data between browsers and servers. The protocol prevents eavesdropping, ensuring that third parties cannot read transmitted data. HTTPS uses public key cryptography to establish secure connections, with servers presenting certificates that browsers verify. SSL/TLS certificates are issued by Certificate Authorities (CAs) that verify the server's identity. The protocol protects against man-in-the-middle attacks by verifying server authenticity. HTTPS encrypts all data including URLs, headers, and content, providing comprehensive protection. The protocol uses port 443 instead of HTTP's port 80, though this is transparent to users. Modern browsers display security indicators like padlocks to show HTTPS connections. HTTPS has become standard for all web traffic, with browsers warning about insecure HTTP sites. The protocol's encryption protects sensitive information like passwords, credit cards, and personal data. HTTPS requires valid certificates, which can be obtained from CAs or through automated services like Let's Encrypt. The protocol's performance overhead is minimal with modern hardware and optimized implementations. HTTPS is essential for web security, protecting users from various attacks. The protocol's widespread adoption has made secure web communication the norm. HTTPS represents a fundamental security control for web applications, protecting both users and data.",
        "OAuth 2.0 is an authorization framework for secure API access, allowing users to grant limited access to their resources without sharing passwords. The framework enables third-party applications to access user data with explicit permission. OAuth 2.0 uses access tokens that represent authorization to access specific resources. The framework supports multiple grant types for different use cases, including authorization code, client credentials, and refresh tokens. OAuth 2.0 separates the roles of resource owner, client application, authorization server, and resource server. The framework's authorization code flow is the most secure, redirecting users to authenticate with the authorization server. OAuth 2.0 tokens have limited lifetimes and scopes, restricting what applications can access. The framework supports refresh tokens that allow obtaining new access tokens without re-authentication. OAuth 2.0 has become the standard for API authorization, used by major platforms like Google, Facebook, and GitHub. The framework enables single sign-on (SSO), allowing users to authenticate once and access multiple services. OAuth 2.0's security depends on proper implementation, including secure token storage and transmission. The framework's flexibility makes it suitable for various scenarios from web apps to mobile apps to server-to-server communication. OAuth 2.0 has revolutionized how applications access user data securely. The framework's widespread adoption has made it essential knowledge for developers building modern applications.",
        "JWT tokens authenticate users in stateless applications, providing a compact way to represent claims between parties. JWTs consist of three parts: header, payload, and signature, encoded as base64url strings separated by dots. The tokens are self-contained, including all necessary information without requiring server-side storage. JWTs use digital signatures to ensure integrity, preventing tampering with token contents. The tokens can include claims about the user, such as identity, roles, and permissions. JWTs are commonly used in REST APIs and microservices architectures where statelessness is important. The tokens' compact format makes them suitable for transmission in URLs, headers, or cookies. JWTs support expiration times, limiting how long tokens remain valid. The tokens can be signed using symmetric algorithms like HS256 or asymmetric algorithms like RS256. JWTs enable distributed authentication, allowing services to verify tokens without contacting an authentication server. The tokens' stateless nature simplifies scaling, as no server-side session storage is required. JWTs must be transmitted securely, typically over HTTPS, to prevent interception. The tokens' contents are base64 encoded but not encrypted, so sensitive data shouldn't be included. JWTs have become standard for API authentication, providing a scalable solution for modern applications. The tokens' flexibility and self-contained nature make them powerful but require careful implementation to maintain security.",
        "SQL injection attacks exploit database vulnerabilities by injecting malicious SQL code through user input. The attacks occur when applications construct SQL queries by concatenating user input without proper sanitization. Attackers can manipulate queries to read, modify, or delete database data. SQL injection can bypass authentication by modifying WHERE clauses to always evaluate to true. The attacks can extract sensitive information like passwords, credit card numbers, and personal data. SQL injection can modify database structure, dropping tables or altering schemas. The attacks can execute system commands on the database server in some configurations. Prevention requires using parameterized queries or prepared statements that separate SQL code from data. Input validation and sanitization help but parameterized queries are the primary defense. SQL injection remains one of the most common web application vulnerabilities despite being well-understood. The attacks can be automated, making them easy to execute at scale. SQL injection has caused major data breaches, compromising millions of records. The OWASP Top 10 consistently lists injection attacks as a critical security risk. Prevention is straightforward with proper coding practices, making SQL injection largely preventable. The attacks' impact can be severe, making them a priority for security testing and code review.",
        "Cross-Site Scripting (XSS) injects malicious scripts into web pages viewed by other users, executing in the context of the vulnerable site. XSS attacks can steal session cookies, allowing attackers to impersonate users. The attacks can modify page content, defacing websites or displaying false information. XSS can redirect users to malicious sites or perform actions on their behalf. Stored XSS persists malicious scripts in the database, affecting all users who view the content. Reflected XSS includes malicious scripts in URLs or form submissions, affecting users who click links. DOM-based XSS manipulates the document object model in the browser, without server involvement. Prevention requires output encoding, ensuring user input is properly escaped before display. Content Security Policy (CSP) headers restrict which scripts can execute, mitigating XSS impact. Input validation helps but output encoding is the primary defense against XSS. XSS attacks are common because many applications don't properly sanitize user input. The attacks can be combined with other techniques for more sophisticated attacks. XSS has been used in major security incidents, compromising user accounts and data. Modern frameworks often provide automatic escaping, but developers must use these features correctly. XSS prevention requires understanding how browsers execute scripts and properly encoding output. The attacks' ability to execute in the context of trusted sites makes them particularly dangerous.",
        "CSRF attacks trick users into unwanted actions by exploiting the trust that sites have in authenticated users. The attacks work by making authenticated users submit requests to sites where they're logged in. CSRF attacks can change passwords, transfer funds, or perform other sensitive actions. The attacks exploit the fact that browsers automatically include cookies with requests to the same domain. CSRF tokens prevent attacks by requiring a secret value that attackers cannot predict. SameSite cookie attributes restrict when cookies are sent, mitigating CSRF risk. Referer headers can be checked to ensure requests originate from the same site. CSRF protection is essential for state-changing operations like POST requests. The attacks are particularly dangerous because they require no user interaction beyond visiting a malicious page. CSRF can be combined with XSS for more sophisticated attacks that bypass some protections. Modern frameworks often include CSRF protection by default, but it must be enabled. The attacks target the trust relationship between users and sites, not technical vulnerabilities. CSRF prevention requires understanding how browsers handle cookies and same-origin policies. The attacks' ability to perform actions as authenticated users makes them a serious security concern. CSRF protection is a fundamental security control for web applications that handle sensitive operations.",
        "Password hashing uses algorithms like bcrypt for security, storing password representations that cannot be reversed. Hashing is one-way, meaning hashes cannot be converted back to original passwords. Secure hashing algorithms like bcrypt, Argon2, and scrypt are designed to be slow, making brute-force attacks impractical. Salting adds random data to passwords before hashing, preventing rainbow table attacks. Each password should have a unique salt, stored alongside the hash. Password hashing prevents attackers from learning passwords even if they gain access to the database. Bcrypt automatically handles salting and includes a cost factor that controls hashing time. Argon2 is the winner of the Password Hashing Competition, designed to resist various attack types. Password hashing must be done server-side, as client-side hashing provides no security benefit. Storing plaintext passwords is a critical security failure that can lead to account compromise. Password hashing libraries handle implementation details, making secure hashing accessible to developers. The practice of password hashing has evolved, with older algorithms like MD5 and SHA-1 being insecure for passwords. Modern password hashing requires understanding algorithm selection, salt generation, and cost factors. Password hashing is a fundamental security practice, essential for protecting user accounts. The practice's importance cannot be overstated, as password breaches can have severe consequences.",
        "Two-factor authentication adds extra security layer by requiring something you know and something you have. 2FA combines passwords (something you know) with a second factor like a code from your phone (something you have). The additional factor makes it much harder for attackers to gain unauthorized access. 2FA can use SMS codes, authenticator apps, hardware tokens, or biometrics as the second factor. Authenticator apps like Google Authenticator generate time-based one-time passwords (TOTP). Hardware tokens provide physical devices that generate codes or use USB/NFC for authentication. SMS-based 2FA is convenient but vulnerable to SIM swapping attacks. Biometric 2FA uses fingerprints, face recognition, or other biological characteristics. 2FA significantly reduces the risk of account compromise even if passwords are stolen. The practice has become standard for sensitive accounts like email, banking, and cloud services. 2FA implementation requires careful design to balance security and usability. Backup codes provide recovery options if the primary 2FA method is unavailable. 2FA can be required for all logins or only for sensitive operations. The practice represents defense in depth, adding layers of security beyond passwords. 2FA has become essential for protecting high-value accounts and sensitive data. The practice's adoption has been driven by increasing awareness of password vulnerabilities and data breaches.",
        "Zero-trust security assumes no implicit trust, requiring verification for every access request regardless of location. The model assumes that threats exist both inside and outside the network perimeter. Zero-trust requires strict identity verification for every user and device attempting to access resources. The model uses least-privilege access, granting only the minimum permissions necessary. Zero-trust continuously monitors and validates security posture, not just at initial access. The model encrypts all traffic and uses micro-segmentation to limit lateral movement. Zero-trust treats all networks as untrusted, whether internal or external. The model requires strong authentication, including multi-factor authentication for all users. Zero-trust uses device trust verification, ensuring devices meet security requirements. The model logs and monitors all access attempts, providing visibility into network activity. Zero-trust architecture has become important as perimeters dissolve with cloud and remote work. The model provides better security than traditional perimeter-based approaches. Zero-trust implementation requires identity management, device management, and network segmentation. The model's principles can be applied gradually, improving security incrementally. Zero-trust represents a fundamental shift in security thinking, from 'trust but verify' to 'never trust, always verify'. The model has become a best practice for modern security architectures, especially in cloud environments.",
        "Security headers protect web applications by instructing browsers to enforce security policies. Content-Security-Policy (CSP) restricts which resources can be loaded, preventing XSS attacks. X-Frame-Options prevents clickjacking by controlling whether pages can be embedded in frames. Strict-Transport-Security (HSTS) forces browsers to use HTTPS, preventing downgrade attacks. X-Content-Type-Options prevents MIME type sniffing, reducing certain attack vectors. Referrer-Policy controls how much referrer information is sent with requests. Permissions-Policy (formerly Feature-Policy) restricts which browser features pages can use. Security headers provide defense in depth, adding layers of protection beyond application code. The headers are easy to implement but provide significant security benefits. Security headers are particularly important for preventing common web vulnerabilities. The headers work by instructing browsers to enforce policies, providing client-side protection. Security headers should be configured based on application requirements and threat model. The headers' effectiveness depends on browser support and proper configuration. Security headers represent low-effort, high-value security improvements. The practice of using security headers has become standard for web applications. Security headers complement other security measures, providing comprehensive protection.",
        "Vulnerability management identifies, assesses, and remediates security weaknesses in systems and applications. The process includes vulnerability scanning to discover potential security issues. Vulnerability assessment evaluates the severity and impact of discovered vulnerabilities. The process prioritizes remediation based on risk, focusing on critical vulnerabilities first. Vulnerability management requires regular scanning, as new vulnerabilities are discovered continuously. The process includes patch management, applying security updates to address vulnerabilities. Vulnerability databases like CVE provide standardized identifiers and information about vulnerabilities. The process requires understanding the difference between vulnerabilities and false positives. Vulnerability management includes tracking remediation progress and verifying fixes. The process should be integrated into development and operations workflows. Vulnerability management helps organizations understand their security posture and prioritize improvements. The process requires balancing security with operational needs and system availability. Automated vulnerability scanning tools can identify many common issues efficiently. Vulnerability management is an ongoing process, not a one-time activity. The practice helps organizations reduce their attack surface and improve security posture. Vulnerability management is essential for maintaining security in complex, evolving systems.",
    ]
    
    service.add_memories_batch(
        texts=memories,
        categories=["security"] * len(memories),
        sources=["documentation"] * len(memories)
    )


def main():
    """Create all sample data."""
    logger.info("ðŸš€ Starting sample data creation...")
    
    service = RagMemoryService(
        auto_create_schema=False,
        similarity_threshold=0.7,  # Lower threshold for more connections
        max_similar_connections=8,  # More connections for interesting graph
        load_graph=True,
    )
    
    # Create different categories of memories
    create_programming_memories(service)
    create_ai_ml_memories(service)
    create_database_memories(service)
    create_web_development_memories(service)
    create_devops_memories(service)
    create_data_science_memories(service)
    create_security_memories(service)
    
    # Get final statistics
    stats = service.get_graph_statistics()
    
    logger.info("\n" + "="*60)
    logger.info("âœ… Sample data creation complete!")
    logger.info("="*60)
    logger.info(f"ðŸ“Š Total memories: {stats['total_memories']}")
    logger.info(f"ðŸ”— Total edges: {stats['total_edges']}")
    logger.info(f"ðŸŒ Connected components: {stats['connected_components']}")
    logger.info(f"ðŸ“ˆ Average degree: {stats['average_degree']:.2f}")
    logger.info(f"ðŸ“‰ Graph density: {stats['graph_density']:.4f}")
    logger.info(f"\nðŸ“ Categories:")
    for category, count in stats['categories'].items():
        logger.info(f"   â€¢ {category}: {count} memories")
    logger.info("="*60)
    logger.info("\nðŸ’¡ Next step: Run visualize_graph.py to create the visualization!")


if __name__ == "__main__":
    main()

